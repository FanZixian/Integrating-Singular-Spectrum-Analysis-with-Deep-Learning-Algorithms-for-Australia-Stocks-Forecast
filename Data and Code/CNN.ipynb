{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 01:12:33.839472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 01:12:34.499665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './'\n",
    "from Window_Generator_package import Window_Generator\n",
    "\n",
    "# stock_example = pd.read_csv(path + 'Data/Grouped/Consumer and Service Sectors/ALL.AX.csv')\n",
    "def read_stock(path, file_name):\n",
    "    stock = pd.read_csv(path + file_name, header = [0])\n",
    "    stock_index = pd.to_datetime(stock['Date'])\n",
    "    stock.index = stock_index\n",
    "    stock = stock.drop('Date', axis = 1)\n",
    "    return stock\n",
    "group = ['Consumer and Service Sectors', 'Financial, Healthcare, Technology, and Utilities Sectors', 'Industrial and Infrastructure Sectors']\n",
    "index = [0, 1, 2]\n",
    "def read_all(the_path):\n",
    "    All_df = []\n",
    "    files = [f for f in os.listdir(the_path) if f.endswith('.csv')]\n",
    "\n",
    "    for name in files:\n",
    "        stock = read_stock(the_path + '/', name)\n",
    "        All_df.append(stock)\n",
    "    return All_df, files\n",
    "All_df_0, files = read_all(path + 'Data/Grouped/' + group[index[0]])\n",
    "stock_example = All_df_0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 1)\n"
     ]
    }
   ],
   "source": [
    "print(stock_example.shape)\n",
    "train_series = stock_example[:1008].squeeze()\n",
    "test_series = stock_example[1008:].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_series_shape: (1008,)\n",
      "test_series_shape: (252,)\n",
      "X_train: (1003, 5, 1)\n",
      "Y_train: (1003, 1, 1)\n",
      "X_test: (252, 5, 1)\n",
      "Y_test: (252, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "w = Window_Generator(train_series = train_series, test_series = test_series)\n",
    "_, _, train_mean, train_std, test_mean, test_std = w.standardization(show=True)\n",
    "_, _, _, _ = w.window_generation(show = True)\n",
    "X_train, X_test, Y_train, Y_test = w.data_getter()\n",
    "train_denormalized = w.denormalize_train()\n",
    "test_denormalized = w.denormalize_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 30\n",
    "\n",
    "def compile_and_fit(model, X_train, Y_train, patience=5):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, epochs=MAX_EPOCHS, verbose = 0)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 01:13:06.416769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:06.542540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:06.543016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:06.545255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:06.545627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:06.545944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:07.427839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:07.428363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:07.428377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-07-31 01:13:07.428791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-07-31 01:13:07.428998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3383 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def cnn_model(window_length):\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv1D(filters=32, kernel_size=window_length, activation='relu', input_shape=(5, 1)))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(64, activation='relu'))\n",
    "    cnn.add(Dense(1, activation = 'linear'))\n",
    "    return cnn\n",
    "\n",
    "cnn = cnn_model(window_length = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = compile_and_fit(cnn, X_train = X_train, Y_train = Y_train)\n",
    "IPython.display.clear_output()\n",
    "train_performance = cnn.evaluate(X_train, Y_train)\n",
    "test_performance = cnn.evaluate(X_test, Y_test)\n",
    "print('Train:', str(train_performance))\n",
    "print('Test:', str(test_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predict = cnn.predict(X_test)\n",
    "Y_predict = Y_predict.reshape(len(Y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As Neural Network doesn't provide the same result each run time, run 5 times and get the average of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_prediction(X_train, Y_train, X_test, Y_test, window_length = 5):\n",
    "    # The input train and test are all 3D list\n",
    "    predicted_train_list = []\n",
    "    predicted_test_list = []\n",
    "    train_MAE_list = []\n",
    "    test_MAE_list = []\n",
    "    for i in range(5):\n",
    "        cnn = cnn_model(window_length)\n",
    "        history = compile_and_fit(cnn, X_train = X_train, Y_train = Y_train)\n",
    "        # IPython.display.clear_output()\n",
    "        train_performance = cnn.evaluate(X_train, Y_train)[1]\n",
    "        test_performance = cnn.evaluate(X_test, Y_test)[1]\n",
    "        # print('Train:', str(train_performance))\n",
    "        # print('Test:', str(test_performance))\n",
    "        Y_predict_train = cnn.predict(X_train)\n",
    "        Y_predict_train = Y_predict_train.reshape(len(Y_predict_train))\n",
    "        predicted_train_list.append(Y_predict_train)\n",
    "\n",
    "        Y_predict_test = cnn.predict(X_test)\n",
    "        Y_predict_test = Y_predict_test.reshape(len(Y_predict_test))\n",
    "        predicted_test_list.append(Y_predict_test)\n",
    "\n",
    "        train_MAE_list.append(train_performance)\n",
    "        test_MAE_list.append(test_performance)\n",
    "\n",
    "    train_MAE = np.mean(train_MAE_list)\n",
    "    test_MAE = np.mean(test_MAE_list)\n",
    "\n",
    "    predicted_train_list = np.array(predicted_train_list)\n",
    "    predicted_train_list = np.mean(predicted_train_list, axis = 0)\n",
    "    predicted_test_list = np.array(predicted_test_list)\n",
    "    predicted_test_list = np.mean(predicted_test_list, axis = 0)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    return predicted_train_list, predicted_test_list, train_MAE, test_MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train_list, predicted_test_list, train_MAE, test_MAE = obtain_prediction(X_train, Y_train, X_test, Y_test, window_length = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_train_list.shape)\n",
    "print(predicted_test_list.shape)\n",
    "print(train_MAE)\n",
    "print(test_MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = w.train_index[w.window_length:]\n",
    "test_index = w.test_index\n",
    "\n",
    "Y_predict_train = pd.Series(predicted_train_list, index = train_index)\n",
    "Y_predict_train = Y_predict_train * train_std[w.window_length:] + train_mean[w.window_length:]\n",
    "Y_actual_train = Y_train.reshape(len(Y_train)) * train_std[w.window_length:] + train_mean[w.window_length:]\n",
    "Y_predict_train = pd.DataFrame(Y_predict_train, index = train_index)\n",
    "Y_actual_train = pd.DataFrame(Y_actual_train, index = train_index)\n",
    "\n",
    "Y_predict_test = pd.Series(predicted_test_list, index = test_index)\n",
    "Y_predict_test = Y_predict_test * test_std + test_mean\n",
    "Y_actual_test = Y_test.reshape(len(Y_test)) * test_std + test_mean\n",
    "Y_predict_test = pd.DataFrame(Y_predict_test, index = test_index)\n",
    "Y_actual_test = pd.DataFrame(Y_actual_test, index = test_index)\n",
    "\n",
    "\n",
    "plt.plot(Y_actual_train, label = 'Train Actual')\n",
    "plt.plot(Y_predict_train, label = 'Train predict')\n",
    "plt.plot(Y_actual_test, label = 'Test Actual')\n",
    "plt.plot(Y_predict_test, label = 'Test predict')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the obtained csv file into Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = './Data/Prediction/CNN/'\n",
    "if os.path.exists(store_path):\n",
    "    print(\"The path exists.\")\n",
    "else:\n",
    "    print(\"The path doesn't exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the code above are examples, here is the training for all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_through all the files inside All_df\n",
    "def obtain_all(the_index, window_length = 5, show = None):\n",
    "  group_name = group[index[the_index]]\n",
    "  All_df, files = read_all(path + 'Data/Grouped/' + group_name)\n",
    "\n",
    "  total_length = len(All_df)\n",
    "  train_All_MAE = []\n",
    "  test_All_MAE = []\n",
    "\n",
    "  for i in range(total_length):\n",
    "    # Obtain the Series\n",
    "    stock = All_df[i]\n",
    "    file_name = files[i]\n",
    "    train_series = stock[:1008].squeeze()\n",
    "    test_series = stock[1008:].squeeze()\n",
    "\n",
    "    # Data Preparation\n",
    "    w = Window_Generator(train_series = train_series, test_series = test_series)\n",
    "    _, _, train_mean, train_std, test_mean, test_std = w.standardization(show=True)\n",
    "    _, _, _, _ = w.window_generation(show = True)\n",
    "    X_train, X_test, Y_train, Y_test = w.data_getter()\n",
    "    train_denormalized = w.denormalize_train()\n",
    "    test_denormalized = w.denormalize_test()\n",
    "\n",
    "    # Training and Prediction\n",
    "    predicted_train_list, predicted_test_list, train_MAE, test_MAE = obtain_prediction(X_train, Y_train, X_test, Y_test, window_length = window_length)\n",
    "    # print('Train_MAE:', train_MAE)\n",
    "    # print('Test_MAE:', test_MAE)\n",
    "    train_All_MAE.append(train_MAE)\n",
    "    test_All_MAE.append(test_MAE)\n",
    "\n",
    "    # Prediction Generation (Are train_set)\n",
    "\n",
    "    train_index = w.train_index[w.window_length:]\n",
    "    test_index = w.test_index\n",
    "\n",
    "    Y_predict_train = pd.Series(predicted_train_list, index = train_index)\n",
    "    Y_predict_train = Y_predict_train * train_std[w.window_length:] + train_mean[w.window_length:]\n",
    "    Y_actual_train = Y_train.reshape(len(Y_train)) * train_std[w.window_length:] + train_mean[w.window_length:]\n",
    "    Y_predict_train = pd.DataFrame(Y_predict_train, index = train_index)\n",
    "    Y_actual_train = pd.DataFrame(Y_actual_train, index = train_index)\n",
    "\n",
    "    Y_predict_test = pd.Series(predicted_test_list, index = test_index)\n",
    "    Y_predict_test = Y_predict_test * test_std + test_mean\n",
    "    Y_actual_test = Y_test.reshape(len(Y_test)) * test_std + test_mean\n",
    "    Y_predict_test = pd.DataFrame(Y_predict_test, index = test_index)\n",
    "    Y_actual_test = pd.DataFrame(Y_actual_test, index = test_index)\n",
    "\n",
    "    # if (show != None):\n",
    "    #   plt.plot(Y_actual_train, label = 'Train Actual')\n",
    "    #   plt.plot(Y_predict_train, label = 'Train predict')\n",
    "    #   plt.plot(Y_actual_test, label = 'Test Actual')\n",
    "    #   plt.plot(Y_predict_test, label = 'Test predict')\n",
    "    #   plt.legend()\n",
    "    #   plt.show()\n",
    "\n",
    "    # Store the results\n",
    "    Y_result = pd.concat([pd.concat([Y_actual_train, Y_predict_train], axis = 1), pd.concat([Y_actual_test, Y_predict_test], axis = 1)], axis = 0)\n",
    "    Y_result.column = ['Actual, Predict']\n",
    "    Y_result.to_csv(store_path + group_name + '/' + file_name)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "\n",
    "  return train_All_MAE, test_All_MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE_1, test_MAE_1 = obtain_all(0, window_length = 5, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE_2, test_MAE_2 = obtain_all(1, window_length = 5, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MAE_3, test_MAE_3 = obtain_all(2, window_length = 5, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = np.column_stack((train_MAE_1, test_MAE_1))\n",
    "train_1 = pd.DataFrame(train_1, columns = ['train', 'test'])\n",
    "train_2 = np.column_stack((train_MAE_2, test_MAE_2))\n",
    "train_2 = pd.DataFrame(train_2, columns = ['train', 'test'])\n",
    "train_3 = np.column_stack((train_MAE_3, test_MAE_3))\n",
    "train_3 = pd.DataFrame(train_3, columns = ['train', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.to_csv(store_path + 'Evaluation' + '/' + 'MAE_group1.csv')\n",
    "train_2.to_csv(store_path + 'Evaluation' + '/' + 'MAE_group2.csv')\n",
    "train_3.to_csv(store_path + 'Evaluation' + '/' + 'MAE_group3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
